\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Durante2010CopulaIntro}
\citation{Genest2009CopulaInFinance,Cherubini2004copula}
\citation{Rychlik2010Reliability}
\citation{Bilgrau2012quantification,Li2011,Kim2008,Ma2012Gini}
\citation{Elidan2013}
\citation{Bilmes98agentle}
\citation{Tewari2011,Bilgrau2016,Bhattacharya2014}
\newlabel{sec:Intro}{{1}{1}{}{section.1}{}}
\citation{Nelsen1999introduction}
\citation{Genest1995Multivariate,Joe1993Multivariate,Kojadinovic2010RpackageMVcopula}
\citation{Bedford2002,Kurowicka2009Book,Czado2010PairCopula}
\citation{Elidan2010,Hanea2006CBN}
\citation{Liu2009}
\citation{Tewari2011}
\citation{Bilgrau2016}
\citation{Bilgrau_Rpackage}
\citation{Rajan2016_GMCM_mixed_data}
\citation{kasa2018}
\citation{Kosmidis2016,Mazo2017,Marbac2017,Rey2012_CopulaMixture}
\citation{Marbac2017}
\newlabel{fig:motivating_example_gmcm}{{1}{2}{(a) A 2-dimensional dataset with 100 samples, (b-c) contours of best-fit GMCM (with 2 components) and GMM (with 3 components), (d-e) 100 random samples generated from the two fitted distributions. A tighter fit and a closer resemblance of the random samples to that of training dataset, suggest the GMCM to be a superior generative model of the data than the GMM}{figure.1}{}}
\newlabel{subsec:LitReview}{{1.1}{2}{}{subsection.1.1}{}}
\citation{Nelsen1999Chapter3}
\citation{Joe1996IFM}
\citation{Bilgrau2016}
\newlabel{subsec:Notation}{{1.2}{3}{}{subsection.1.2}{}}
\newlabel{sec:GMC_description}{{2}{3}{}{section.2}{}}
\newlabel{eq:GMC_density}{{1}{3}{}{equation.2.1}{}}
\newlabel{subsec:MLE_GMCM}{{2.1}{3}{}{subsection.2.1}{}}
\newlabel{eq:gmcm_logL}{{2}{3}{}{equation.2.2}{}}
\newlabel{eq:invCDF_GMM}{{3}{3}{}{equation.2.3}{}}
\citation{Tewari2011,Bilgrau2016}
\newlabel{eq:GMCM_density}{{4}{4}{}{equation.3.4}{}}
\citation{diez2003}
\citation{Stephens2000}
\citation{White1982}
\citation{Bilgrau2016}
\newlabel{fig:gmcm_transformation}{{2}{5}{Illustration of the transformations induced by a GMCM. The left panel shows the density contours of a 2-component GMM with parameters $\bs {\alpha } = \{0.45, \ 0.55\}$, $\bs {\mu } =\{[2 \ 5], [7 \ 3] \}$ and $ \Sigma = \{[1.5 \ -1.3 ; -1.3 \quad 3 ],[3 \quad 1.2 ; 1.2 \quad 1] \}$. The middle panel shows the contours under the transformation by the marginal distribution functions [equation \eqref {eq:marginal_CDF}]. The right panel shows the transformation by the quantile functions of $Lognormal(0,0.5)$ and $Beta(10,2)$ distributions along $x$ and $y$ dimensions, respectively [equation \eqref {eq:marginal_iCDF}]. Note that $\bs {x}\in \mathbb {R}^+\times [0,\ 1]$ owing to the Lognormal and Beta marginals}{figure.2}{}}
\newlabel{sec:identifiability_GMCM}{{4}{5}{}{section.4}{}}
\newlabel{eq:true_GMC_density}{{5}{5}{}{equation.4.5}{}}
\newlabel{eq:transformed_GMC_density}{{6}{5}{}{equation.4.6}{}}
\citation{Bilmes98agentle}
\citation{Salakhutdinov2002}
\citation{Tewari2011,Bhattacharya2014}
\citation{Bilmes98agentle}
\newlabel{sec:EM}{{5}{6}{}{section.5}{}}
\newlabel{subsec:EStep}{{5.1}{6}{}{subsection.5.1}{}}
\newlabel{eq:gmcm_complelte_logL}{{7}{6}{}{equation.5.7}{}}
\newlabel{eq:GEM_objective}{{8}{6}{}{equation.5.8}{}}
\citation{Bilgrau2012quantification,Wang2014,Yu2013GMCMWindPred,Bayestehtashk2015}
\citation{Neath1997}
\citation{Bhattacharya2014}
\citation{Tewari2011}
\newlabel{sec:Experimental}{{6}{7}{}{section.6}{}}
\newlabel{fig:identifiability_exp}{{3}{7}{Manifestation of identifiability in GMC distribution shown empirically. Both plots show the evolution of the mean parameters ($\mu _i$), from a same initial point until convergence (2000 iterations), with and without regularization via identifiability constraints put forth in Theorem 1. The true values of the parameters are shown by green squares}{figure.3}{}}
\citation{Iwata2012}
\citation{Lawrence2003}
\citation{Rasmussen1999}
\citation{Duvenaud2013}
\citation{Iwata2012}
\bibdata{citations}
\bibstyle{icml2022}
\newlabel{fig:EM_algo_comp}{{4}{8}{(a) log-likelihood vs. iteration for the three EM algorithms on a simulated dataset (b) Box-plots of converged log-likelihood ratios, $\log \left ( \frac {\mathcal {L}\left (\Theta ^{GEM}|U\right )}{\mathcal {L}\left (\Theta ^{PEM_1}|U\right )}\right )$ and $\log \left ( \frac {\mathcal {L}\left (\Theta ^{GEM}|U\right )}{\mathcal {L}\left (\Theta ^{PEM_2}|U\right )}\right )$, by repeating this experiment on 100 such simulated datasets. The sub-optimal performance of pseudo-EM (PEM) algorithms is clearly evident}{figure.4}{}}
\newlabel{tab:public_datasets}{{1}{8}{Summary of datasets used for comparing different density estimators. The datasets are procured from a public database, ODDS, that provides access to a large collection datasets to data science community for research purpose}{table.1}{}}
\newlabel{apd:symbol_glossary}{{A}{9}{\refname }{appendix.A}{}}
\newlabel{tab:symbol_glossary}{{2}{9}{Symbols and descriptions}{table.2}{}}
\newlabel{eq:ll_comp_expectation}{{9}{9}{\refname }{equation.A.9}{}}
\newlabel{eq:der_z}{{11}{10}{\refname }{equation.A.11}{}}
\gdef \@abspage@last{10}
