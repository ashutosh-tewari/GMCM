{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow-probability==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic command for automatic reload of python modules without needing to restart the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd=tfp.distributions\n",
    "tfb=tfp.bijectors\n",
    "from scipy import io\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import mixture\n",
    "import joblib as jbl\n",
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from mixture_models import GMCM, GMC\n",
    "import utils_v1 as utl\n",
    "\n",
    "# adding path to the data folder\n",
    "sys.path.append('real_world_data/')\n",
    "from gas import GAS\n",
    "from power import POWER\n",
    "from miniboone import MINIBOONE\n",
    "from hepmass import HEPMASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples (before downsampling) = 525123, Number of dimensions = 21\n",
      "Number of training samples (after downsampling) = 10042, Number of dimensions = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 12:23:43.521191: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-01-11 12:23:43.521257: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-16-106-26.ec2.internal): /proc/driver/nvidia/version does not exist\n",
      "2023-01-11 12:23:43.526147: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Marginals\n",
      "Marginals learnt in 236.67 s.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "@ Iter:0,                         Training error: -4.9,                         Validation error: nan,                         Time Elapsed: 30.1 s\n",
      "@ Iter:500,                         Training error: -4.8,                         Validation error: nan,                         Time Elapsed: 102.5 s\n",
      "@ Iter:1000,                         Training error: -5.1,                         Validation error: nan,                         Time Elapsed: 178.4 s\n",
      "@ Iter:1500,                         Training error: -4.8,                         Validation error: nan,                         Time Elapsed: 256.2 s\n",
      "@ Iter:2000,                         Training error: -4.2,                         Validation error: nan,                         Time Elapsed: 327.9 s\n",
      "@ Iter:2500,                         Training error: -5.6,                         Validation error: nan,                         Time Elapsed: 400.0 s\n",
      "@ Iter:3000,                         Training error: -5.3,                         Validation error: nan,                         Time Elapsed: 469.6 s\n",
      "@ Iter:3500,                         Training error: -4.8,                         Validation error: nan,                         Time Elapsed: 540.0 s\n",
      "@ Iter:4000,                         Training error: -5.0,                         Validation error: nan,                         Time Elapsed: 608.1 s\n",
      "@ Iter:4500,                         Training error: -5.0,                         Validation error: nan,                         Time Elapsed: 676.7 s\n",
      "@ Iter:5000,                         Training error: -5.4,                         Validation error: nan,                         Time Elapsed: 745.9 s\n",
      "@ Iter:5500,                         Training error: -5.0,                         Validation error: nan,                         Time Elapsed: 812.3 s\n",
      "@ Iter:6000,                         Training error: -4.9,                         Validation error: nan,                         Time Elapsed: 877.0 s\n",
      "@ Iter:6500,                         Training error: -5.3,                         Validation error: nan,                         Time Elapsed: 944.5 s\n",
      "@ Iter:7000,                         Training error: -5.7,                         Validation error: nan,                         Time Elapsed: 1010.4 s\n",
      "@ Iter:7500,                         Training error: -5.4,                         Validation error: nan,                         Time Elapsed: 1077.4 s\n",
      "@ Iter:8000,                         Training error: -4.1,                         Validation error: nan,                         Time Elapsed: 1141.6 s\n",
      "@ Iter:8500,                         Training error: -4.9,                         Validation error: nan,                         Time Elapsed: 1206.7 s\n",
      "@ Iter:9000,                         Training error: -4.6,                         Validation error: nan,                         Time Elapsed: 1274.1 s\n",
      "@ Iter:9500,                         Training error: -5.2,                         Validation error: nan,                         Time Elapsed: 1338.3 s\n",
      "{'HEPMASS': array([-22.96475029])}\n"
     ]
    }
   ],
   "source": [
    "# data_sets = ['GAS','POWER','MINIBOONE']\n",
    "data_sets = ['HEPMASS']\n",
    "\n",
    "n_reps,mini_batch,max_training_iters = 1,50,10000\n",
    "ll_list = []\n",
    "\n",
    "density_estimation={} #initializing dict to \n",
    "for i in range(len(data_sets)):\n",
    "    if data_sets[i] == 'GAS':\n",
    "        gg=GAS()\n",
    "    elif data_sets[i] == 'POWER':\n",
    "        gg=POWER()\n",
    "    elif data_sets[i] == 'HEPMASS':\n",
    "        gg=HEPMASS()\n",
    "    elif data_sets[i] == 'MINIBOONE':\n",
    "        gg=MINIBOONE()\n",
    "\n",
    "    data_trn = gg.trn.x\n",
    "    data_tst = gg.tst.x\n",
    "    data_vld = gg.val.x\n",
    "    data_all = np.concatenate([data_trn,data_tst,data_vld],axis=0).astype('float32')\n",
    "    print(f'Number of samples (before downsampling) = {data_all.shape[0]}, Number of dimensions = {data_all.shape[1]}')\n",
    "    \n",
    "    # gathering extreme points for each dimension (to be included in the training set)\n",
    "    extreme_points=np.concatenate([np.argmin(data_all,axis=0), np.argmax(data_all,axis=0)])\n",
    "    \n",
    "    log_like_test=np.zeros(n_reps)\n",
    "    for j in range(n_reps):\n",
    "        data_trn=data_all[extreme_points,:] # initializing training set with extreme points\n",
    "        np.random.seed(j)\n",
    "        rand_id=np.random.permutation(data_all.shape[0])\n",
    "        trn_id,vld_id,tst_id,_=np.split(rand_id,[10000,11000,12000])\n",
    "        data_trn=np.concatenate((data_trn,data_all[trn_id,:]),axis=0) \n",
    "        data_vld=data_all[vld_id,:]\n",
    "        data_tst=data_all[tst_id,:]\n",
    "\n",
    "        nsamps,ndims = data_trn.shape\n",
    "        print(f'Number of training samples (after downsampling) = {data_trn.shape[0]}, Number of dimensions = {ndims}')\n",
    "\n",
    "        # Defining a preprocessing bijective transform (eg, log transform in this case)  \n",
    "        min_val = (np.min(data_all)-3*np.std(data_all)).astype('float32')\n",
    "        log_transform = tfb.Chain([tfb.Shift(shift=min_val),tfb.Exp()])\n",
    "\n",
    "        # Training GMCM with data transformation\n",
    "        # Initialing GMCM object and fitting the distribution \n",
    "        gmcm_obj=GMCM(ndims, data_transform=log_transform)\n",
    "        nll_train, nll_vld=gmcm_obj.fit_dist_IFM(data_trn,\n",
    "                                                 n_comps=40,\n",
    "                                                 #data_vld=data_vld,\n",
    "                                                 batch_size=mini_batch,\n",
    "                                                 max_iters=max_training_iters,\n",
    "                                                 regularize=True,\n",
    "                                                 print_interval=500,\n",
    "                                                 init = 'gmm', \n",
    "                                                 optimizer = tf.optimizers.Adam(learning_rate=1E-3))\n",
    "\n",
    "\n",
    "#         # identifying the best iteration from validation error\n",
    "#         non_nan_ids=[e for e in range(len(nll_vld)) if np.isfinite(nll_vld[e])]\n",
    "#         nll_vld_smooth=utl.moving_average(nll_vld[non_nan_ids],5)\n",
    "#         best_training_iters=non_nan_ids[np.argmin(nll_vld_smooth)+5]\n",
    "\n",
    "#         # Training GMCM again with the specified number of iteration\n",
    "#         # Initialing GMCM object and fitting the distribution \n",
    "#         gmcm_obj_best_fit=GMCM(ndims, data_transform=log_transform)\n",
    "#         nll_train, nll_vld=gmcm_obj_best_fit.fit_dist_IFM(data_trn,\n",
    "#                                                           n_comps=40,\n",
    "#                                                           batch_size=mini_batch,\n",
    "#                                                           max_iters=best_training_iters,\n",
    "#                                                           optimizer = tf.optimizers.Adam(learning_rate=1E-4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        log_like_test[j]=np.mean(gmcm_obj.distribution.log_prob(data_tst))\n",
    "        density_estimation[data_sets[i]]=log_like_test\n",
    "        print(density_estimation)\n",
    "#         jbl.dump(density_estimation,'results/DE_results_gmcm_gmm_init_hepmass')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nll_train/100)\n",
    "plt.plot(nll_vld/1000,'ks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_vld=gmcm_obj.distribution.log_prob(data_vld).numpy()\n",
    "ll_tst=gmcm_obj.distribution.log_prob(data_tst).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean(ll_tst),np.mean(ll_vld)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_ids=[i for i in range(len(nll_vld)) if np.isfinite(nll_vld[i])]\n",
    "nll_vld_smooth=utl.moving_average(nll_vld[non_nan_ids],5)\n",
    "best_num_iter=non_nan_ids[np.argmin(nll_vld_smooth)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_training_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isfinite(nll_vld[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_vld[1]==np.nan.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nll_vld[:200],'ks');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg.val.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power import POWER\n",
    "gg=POWER('C:/Users/tewar/Documents/work/GMCM/data/power/data.npy')\n",
    "data = gg.trn.x\n",
    "nsamps,ndims = data.shape\n",
    "idx_selected = np.unique(np.random.randint(0,nsamps,int(nsamps/50)))\n",
    "data_in = data[idx_selected,:].astype('float32')\n",
    "# data_in = np.random.randn(1000,10).astype('float32')\n",
    "min_val = np.min(data_in).astype('float32')-1\n",
    "shift_exp_bijec = tfb.Chain([tfb.Shift(shift=min_val.astype('float32')),tfb.Exp()])\n",
    "gmcm_obj = GMCM(ndims, data_in, forward_transform=shift_exp_bijec)\n",
    "ll_trn=gmcm_obj.fit_GMC_dist(5,max_iters=5,batch_size=50,initialization=['random',1], print_interval=500, regularize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl,vals = [0, 2, 3, 5], [0.1, -2, 1.2, 3.4]\n",
    "marg_gmcm = gmcm_obj.get_marginal(dl)\n",
    "cond_gmcm = gmcm_obj.get_conditional(dl,vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_all = np.array([0.1, 0, -2, 1.2, 0, 3.4]).astype('float32').reshape(1,-1)\n",
    "vv_obs = np.array(vals).astype('float32').reshape(1,-1)\n",
    "vv_unobs = np.array([0.,0.]).astype('float32').reshape(1,-1)\n",
    "print(gmcm_obj.distribution.log_prob(vv_all) - marg_gmcm.distribution.log_prob(vv_obs))\n",
    "print(cond_gmcm.distribution.log_prob(vv_unobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = vec2gmm_params(gmcm_obj.ndims,gmcm_obj.ncomps,gmcm_obj.gmc.params)\n",
    "out2 = vec2gmm_params(marg_gmcm.ndims, marg_gmcm.ncomps,marg_gmcm.gmc.params)\n",
    "out3 = vec2gmm_params(cond_gmcm.ndims, cond_gmcm.ncomps,cond_gmcm.gmc.params)\n",
    "\n",
    "\n",
    "print(tf.math.softmax(out1[0]))\n",
    "print(tf.math.softmax(out2[0]))\n",
    "print(tf.math.softmax(out3[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = gmcm_obj.distribution.sample(10000)\n",
    "data2 = marg_gmcm.distribution.sample(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv =np.array([1,1.,1]).astype('float32').reshape(1,-1)\n",
    "vv1 = marg_gmcm.distribution.bijector.inverse(vv)\n",
    "print(vv1)\n",
    "vv2 = marg_gmcm.distribution.distribution.bijector.inverse(vv1)\n",
    "print(vv2)\n",
    "vv3 = marg_gmcm.distribution.distribution.distribution.bijector.inverse(vv2)\n",
    "print(vv3)\n",
    "\n",
    "obj = marg_gmcm\n",
    "while hasattr(obj.distribution,'bijector'):\n",
    "    vv = obj.distribution.bijector.inverse(vv).numpy()\n",
    "    print(vv)\n",
    "    obj = obj.distribution\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data1[:,3],data1[:,0],'k.');\n",
    "plt.plot(data2[:,1],data2[:,0],'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(ll_trn+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ll_1)\n",
    "np.mean(ll_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmcm_obj.gmc.total_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = gmcm_obj.distribution.sample(2000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1,id2 = np.random.choice(ndims,2)\n",
    "plt.subplot(121)\n",
    "plt.plot(ss[:,id1],ss[:,id2],'k.')\n",
    "plt.subplot(122)\n",
    "plt.plot(data_in[:,id1],data_in[:,id2],'k.')\n",
    "print([id1,id2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(ss[:,id2],20);\n",
    "plt.hist(data_in[:,id2],50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(data_in[:,1]+1),50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../logs/GAS/LRminus3_NComps60/chkpt/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_chain(init_state,step_size,target_log_prob_fn, unconstraining_bijectors=tfb.Identity(),num_steps=1000,burnin=50):\n",
    "    def trace_fn(_,pkr):\n",
    "        return (\n",
    "            pkr.inner_results.inner_results.target_log_prob,\n",
    "            pkr.inner_results.inner_results.leapfrogs_taken,\n",
    "            pkr.inner_results.inner_results.has_divergence,\n",
    "            pkr.inner_results.inner_results.energy,\n",
    "            pkr.inner_results.inner_results.log_accept_ratio\n",
    "                )\n",
    "#     kernel = tfp.mcmc.TransformedTransitionKernel(\n",
    "#         inner_kernel=tfp.mcmc.NoUTurnSampler(\n",
    "#             target_log_prob_fn,\n",
    "#             step_size=step_size),\n",
    "#         bijector=unconstraining_bijectors)\n",
    "\n",
    "#     hmc = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "#         inner_kernel=kernel,\n",
    "#         num_adaptation_steps=burnin,\n",
    "#         step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n",
    "#               inner_results=pkr.inner_results._replace(step_size=new_step_size)),\n",
    "#         step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,\n",
    "#         log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio\n",
    "#       )\n",
    "    \n",
    "    \n",
    "    hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=tfp.mcmc.RandomWalkMetropolis(target_log_prob_fn),\n",
    "        bijector=unconstraining_bijectors) \n",
    "    \n",
    "    \n",
    "\n",
    "    # Sampling from the chain.\n",
    "    return tfp.mcmc.sample_chain(\n",
    "        num_results=num_steps,\n",
    "        num_burnin_steps=burnin,\n",
    "        current_state=init_state,\n",
    "        kernel=hmc)\n",
    "#         trace_fn=trace_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=np.load('../logs/GAS/LRminus3_NComps60/chkpt/iter10000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aa)\n",
    "aa[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.1,0.2,0.7]\n",
    "mu = np.random.randn(3,2).astype('float32')\n",
    "sig = np.zeros((3,2,2)).astype('float32')\n",
    "sig[0] = tfb.FillScaleTriL(diag_bijector=tfb.Exp()).forward(np.random.randn(3).astype('float32'))\n",
    "sig[1] = tfb.FillScaleTriL(diag_bijector=tfb.Exp()).forward(np.random.randn(3).astype('float32'))\n",
    "sig[2] = tfb.FillScaleTriL(diag_bijector=tfb.Exp()).forward(np.random.randn(3).astype('float32'))\n",
    "dist=tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "                          components_distribution=tfd.MultivariateNormalTriL(loc=mu,scale_tril=sig))\n",
    "ss = dist.sample(10000).numpy()\n",
    "tf.reduce_mean(dist.log_prob(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmcm_obj = GMCM(2, ss)\n",
    "ll_trn=gmcm_obj.fit_GMC_dist(2,max_iters=5000,batch_size=50,initialization=['random',0], print_interval=500, regularize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ll_trn)\n",
    "tf.reduce_mean(gmcm_obj.distribution.log_prob(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmcm_obj.gmc.distribution.log_prob(np.array([0.3,0.1]).astype('float32').reshape(1,-1))\n",
    "\n",
    "def target_log_prob(u_part):\n",
    "    u = tf.concat([u_part,tf.constant(0.999999,shape=(1,))],axis=0)\n",
    "    u = tf.reshape(u,shape=(1,-1))\n",
    "    return gmcm_obj.gmc.distribution.log_prob(u)\n",
    "\n",
    "init_state = tf.constant(np.random.rand(1).astype('float32'))\n",
    "target_log_prob(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_state = tf.constant(np.random.rand(2).astype('float32'))\n",
    "u_init = tf.Variable(init_state)\n",
    "with tf.GradientTape() as tape:\n",
    "    out = target_log_prob(u_init)\n",
    "grads = tape.gradient(out, u_init)\n",
    "print(out)\n",
    "print(grads)\n",
    "\n",
    "grad_fd = gradientFiniteDifferent(target_log_prob,u_init,delta=1E-4)\n",
    "print(grad_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running 5 chains in parallel\n",
    "n_chains = 5\n",
    "init_state = tf.constant(np.random.rand(2).astype('float32'))\n",
    "step_size= 0.1\n",
    "# bijector to map contrained parameters to real\n",
    "ts = time.time()\n",
    "output_NUTS = run_chain(init_state, step_size, target_log_prob,num_steps=10,burnin=50)\n",
    "print(f'NUTS runtime for {n_chains} chains: {time.time()-ts} s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = tfp.mcmc.effective_sample_size(output_NUTS.all_states)\n",
    "ess = tfp.transpose(ess).numpy()\n",
    "plot(ess)\n",
    "total_samples_all_chains = np.prod(output_NUTS.all_states.shape[:2])\n",
    "total_samples = tf.reshape(output_NUTS.all_states,shape=(total_samples_all_chains,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_NUTS.all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_vec = np.linspace(0.001,0.9999,50).astype('float32').reshape(-1,1)\n",
    "p_vec = np.zeros(50)\n",
    "for i in range(50):\n",
    "    p_vec[i] = target_log_prob(u_vec[i])\n",
    "    if i%10 ==0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(u_vec,tf.exp(p_vec),'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Numerically finding the icdf values for a distribution whos analytical CDF is specified\n",
    "# def icdf_numerical(u,cdf_funct,lb,ub):\n",
    "#     # setting up the numerical method (Chandrupatla root finding algorithm) to find icdf\n",
    "#     obj_func = lambda x: cdf_funct(x) - u\n",
    "#     # finding the roots\n",
    "#     x = tfp.math.find_root_chandrupatla(obj_func,low=lb,high=ub)[0]\n",
    "#     return x\n",
    "\n",
    "# # Standardize GMM parameters\n",
    "# def standardize_gmm_params(alphas,mus,covs):\n",
    "#     weighted_mus = tf.linalg.matvec(tf.transpose(mus),alphas)\n",
    "#     new_mus = mus - weighted_mus\n",
    "#     variances = tf.linalg.diag_part(covs)\n",
    "#     scaling_vec = tf.linalg.matvec(tf.transpose(new_mus**2+variances),alphas)\n",
    "#     scaling_matrix = tf.linalg.diag(1/(scaling_vec**0.5))\n",
    "#     new_mus = tf.linalg.matmul(new_mus,scaling_matrix)\n",
    "#     new_covs = tf.linalg.matmul(covs,scaling_matrix**2)\n",
    "#     return alphas,new_mus,new_covs\n",
    "\n",
    "# def vec2gmm_params(n_dims,n_comps,param_vec):\n",
    "#     num_alpha_params = n_comps\n",
    "#     num_mu_params = n_comps*n_dims\n",
    "#     num_sig_params = int(n_comps*n_dims*(n_dims+1)*0.5)\n",
    "#     logit_param, mu_param, chol_param = tf.split(param_vec,[num_alpha_params,num_mu_params,num_sig_params])\n",
    "#     mu_vectors = tf.reshape(mu_param, shape=(n_comps,n_dims))\n",
    "#     chol_mat_array=tf.TensorArray(tf.float32,size=n_comps)\n",
    "#     cov_mat_array=tf.TensorArray(tf.float32,size=n_comps)\n",
    "#     for k in range(n_comps):\n",
    "#         start_idx = tf.cast(k*(num_sig_params/n_comps),tf.int32)\n",
    "#         end_idx = tf.cast((k+1)*(num_sig_params/n_comps),tf.int32)\n",
    "#         chol_mat = tfb.FillScaleTriL(diag_bijector=tfb.Exp()).forward(chol_param[start_idx:end_idx])\n",
    "#         cov_mat = tf.matmul(chol_mat,tf.transpose(chol_mat))\n",
    "#         chol_mat_array = chol_mat_array.write(k,chol_mat) \n",
    "#         cov_mat_array =  cov_mat_array.write(k,cov_mat) \n",
    "        \n",
    "#     chol_matrices = chol_mat_array.stack()\n",
    "#     cov_matrices = cov_mat_array.stack()     \n",
    "#     return [logit_param,mu_vectors,cov_matrices,chol_matrices]\n",
    "\n",
    "# def gmm_params2vec(n_dims,n_comps,alphas,mu_vectors,cov_matrices):\n",
    "#     # now gathering all the parameters into a single vector\n",
    "#     param_list = []\n",
    "#     param_list.append(np.log(alphas))\n",
    "#     param_list.append(tf.reshape(mu_vectors,-1))\n",
    "#     for k in range(n_comps):\n",
    "#         chol_mat = tf.linalg.cholesky(cov_matrices[k])\n",
    "#         param_list.append(tfb.FillScaleTriL(diag_bijector=tfb.Exp()).inverse(chol_mat))\n",
    "#     param_vec = tf.concat(param_list,axis=0)\n",
    "#     return param_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GMC bijector\n",
    "# class GMC_bijector(tfb.Bijector):\n",
    "#     def __init__(self,n_dims,n_comps,param_list,forward_min_event_ndims=1, validate_args: bool = False,name=\"gmc\"):\n",
    "#         super(GMC_bijector, self).__init__(\n",
    "#             validate_args=validate_args, forward_min_event_ndims=forward_min_event_ndims, name=name\n",
    "#         )\n",
    "        \n",
    "#         assert (len(param_list)==3), 'incorrect number of inputs'\n",
    "#         assert param_list[1].shape == [n_comps,n_dims], 'the dimension of mean vectors should be ncomps x ndims'\n",
    "#         assert param_list[2].shape == [n_comps,n_dims], 'the dimension of variance vectors should be ncomps x ndims'\n",
    "        \n",
    "#         self.ndims = n_dims\n",
    "#         self.ncomps = n_comps\n",
    "#         self.logits = param_list[0]\n",
    "#         self.mu_vectors = param_list[1]\n",
    "#         self.var_vectors = param_list[2]\n",
    "#         self.std_vectors = self.var_vectors**0.5\n",
    "    \n",
    "#     def _forward(self, x_mat):\n",
    "#         assert x_mat.shape[1] == self.ndims, 'expected data dimensions n_samps x n_dims'\n",
    "#         dist = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(logits=self.logits),\n",
    "#                                components_distribution=tfd.Normal(loc=tf.transpose(self.mu_vectors),\n",
    "#                                                                   scale=tf.transpose(self.std_vectors)))\n",
    "#         u_mat = dist.cdf(x_mat)\n",
    "#         return u_mat\n",
    "    \n",
    "#     def _inverse(self, u_mat):\n",
    "#         assert u_mat.shape[1] == self.ndims, 'expected data dimensions n_samps x n_dims'\n",
    "#         x_mat = self.gmm_icdf_parallel(u_mat,self.logits,tf.transpose(self.mu_vectors),tf.transpose(self.std_vectors))\n",
    "#         return x_mat\n",
    "    \n",
    "#     def _inverse_log_det_jacobian(self, u_mat):\n",
    "#         x_mat = self._inverse(u_mat)\n",
    "#         dist = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(logits=self.logits),\n",
    "#                                components_distribution=tfd.Normal(loc=tf.transpose(self.mu_vectors),\n",
    "#                                                                   scale=tf.transpose(self.std_vectors)))\n",
    "#         log_det_J_mat = dist.log_prob(x_mat)\n",
    "#         return -tf.reduce_sum(log_det_J_mat,axis=1)    \n",
    "    \n",
    "#     # Numerically finding the icdf values of univariate gmm distributions (one along each dimension)\n",
    "#     @tf.custom_gradient\n",
    "#     def gmm_icdf_parallel(self,u_mat,logit,mu_T,std_T):\n",
    "#         # Setting up the numerical method to find icdf\n",
    "#         # first define a function that computes the residual between the actual true CDF values and the CDF value as specified matrix\n",
    "#         obj_func = lambda x: tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(logits=logit),\n",
    "#                                                    components_distribution=tfd.Normal(loc=mu_T,scale=std_T)).cdf(x)-u_mat\n",
    "#         # specifying the lower and upper bounds of the root based on current parameters\n",
    "#         lb = tf.reduce_min(mu_T,axis=1) - 5*tf.reduce_max(std_T,axis=1)\n",
    "#         ub = tf.reduce_max(mu_T,axis=1) + 5*tf.reduce_max(std_T,axis=1)\n",
    "#         # replicating the lower and upper bounds\n",
    "#         lb = tf.repeat(tf.reshape(lb,[1,-1]),u_mat.shape[0],axis=0)\n",
    "#         ub = tf.repeat(tf.reshape(ub,[1,-1]),u_mat.shape[0],axis=0)\n",
    "#         # finding the roots (Chandrupatla root finding algorithm)\n",
    "#         x_mat = tfp.math.find_root_chandrupatla(obj_func,low=lb,high=ub)[0]\n",
    "#         # following code implements custom gradient\n",
    "#         def grad(dy):\n",
    "#             # Calling  another python function to get the partial derivatives\n",
    "#             grad_logit, grad_mu, grad_std = self.partial_deriv_z(x_mat,logit,mu_T,std_T)\n",
    "\n",
    "#             temp_mat = tf.linalg.matmul(grad_logit,dy)\n",
    "\n",
    "#             logit_grad = tf.linalg.diag_part(temp_mat)\n",
    "#             logit_grad = tf.reduce_sum(logit_grad,axis=1)\n",
    "\n",
    "#             temp_mat = tf.linalg.matmul(grad_mu,dy)\n",
    "#             mu_grad = tf.linalg.diag_part(temp_mat)\n",
    "\n",
    "#             temp_mat = tf.linalg.matmul(grad_std,dy)\n",
    "#             std_grad = tf.linalg.diag_part(temp_mat)\n",
    "            \n",
    "#             return tf.constant(0.,shape=(u_mat.shape)), logit_grad, tf.transpose(mu_grad), tf.transpose(std_grad)    \n",
    "#         return x_mat, grad\n",
    "    \n",
    "#     # Analytical partial derivative of icdf of Gaussian Mixture marginals\n",
    "#     def partial_deriv_z(self,z,logit,mu_T,std_T):\n",
    "#         alpha = tf.math.softmax(logit)\n",
    "#         grad_logit_array = tf.TensorArray(tf.float32, size=self.ncomps)\n",
    "#         grad_mu_array = tf.TensorArray(tf.float32, size=self.ncomps)\n",
    "#         grad_var_array = tf.TensorArray(tf.float32, size=self.ncomps)        \n",
    "#         dist = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(logits=logit),\n",
    "#                                 components_distribution=tfd.Normal(loc=mu_T,\n",
    "#                                                                    scale=std_T))\n",
    "#         common_factor1 = dist.prob(z)\n",
    "#         for k in range(self.ncomps):        \n",
    "#             common_factor2 = tfd.Normal(loc=mu_T[:,k],scale=std_T[:,k]).prob(z)\n",
    "#             term = 0.5*(1+tf.math.erf((z-mu_T[:,k])/(tf.math.sqrt(2.)*std_T[:,k])))\n",
    "#             v1 = -alpha[k]*(term - dist.cdf(z))/common_factor1\n",
    "#             v2 = alpha[k]*common_factor2/common_factor1\n",
    "#             v3 = v2 * ((z-mu_T[:,k])/(std_T[:,k])) \n",
    "\n",
    "#             grad_logit_array = grad_logit_array.write(k, tf.transpose(v1) )\n",
    "#             grad_mu_array = grad_mu_array.write(k, tf.transpose(v2) )\n",
    "#             grad_var_array = grad_var_array.write(k, tf.transpose(v3) )\n",
    "#         return grad_logit_array.stack(), grad_mu_array.stack(), grad_var_array.stack()\n",
    "    \n",
    "#  # Marignal transform bijector\n",
    "# class Marginal_transform(tfb.Bijector):\n",
    "#     def __init__(self,ndims,marg_dist_list,forward_min_event_ndims=1, validate_args: bool = False,name=\"marginals\"):\n",
    "#         super(Marginal_transform, self).__init__(\n",
    "#             validate_args=validate_args, forward_min_event_ndims=forward_min_event_ndims, name=name\n",
    "#         )\n",
    "#         self.ndims = ndims\n",
    "#         self.marg_dists = marg_dist_list\n",
    "    \n",
    "#     def _inverse(self, x_mat):\n",
    "#         nobs = x_mat.get_shape().as_list()[0]\n",
    "#         temp_array = tf.TensorArray(tf.float32,size=self.ndims)\n",
    "#         for j in range(self.ndims):\n",
    "#             u_cur = self.marg_dists[j]['cdf'](x_mat[:,j])\n",
    "#             temp_array = temp_array.write(j,u_cur)\n",
    "#         u_mat = tf.transpose(temp_array.stack())            \n",
    "#         return u_mat\n",
    "    \n",
    "#     def _forward(self, u_mat):\n",
    "#         temp_array = tf.TensorArray(tf.float32,size=self.ndims)\n",
    "#         for j in range(self.ndims):\n",
    "#             x_cur = icdf_numerical(u_mat[:,j], self.marg_dists[j]['cdf'],self.marg_dists[j]['lb'],self.marg_dists[j]['ub'])\n",
    "#             temp_array = temp_array.write(j,x_cur)\n",
    "#         x_mat = tf.transpose(temp_array.stack())              \n",
    "#         return x_mat\n",
    "    \n",
    "#     def _forward_log_det_jacobian(self, u_mat):\n",
    "#         x_mat = self._forward(u_mat)\n",
    "#         temp_array = tf.TensorArray(tf.float32,size=self.ndims)\n",
    "#         for j in range(self.ndims):\n",
    "#             temp_array = temp_array.write(j,self.marg_dists[j]['log_pdf'](x_mat[:,j]))\n",
    "#         log_det_J_mat = tf.transpose(temp_array.stack())\n",
    "#         return -tf.reduce_sum(log_det_J_mat,axis=1) \n",
    "    \n",
    "#     def _inverse_log_det_jacobian(self, x_mat):\n",
    "#         u_mat = self._inverse(x_mat)\n",
    "#         return -self._forward_log_det_jacobian(u_mat)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Defining GMC class\n",
    "# class GMC:\n",
    "#     def __init__(self, n_dims, n_comps, param_vec):        \n",
    "#         self.ndims = n_dims\n",
    "#         self.ncomps = n_comps   \n",
    "#         self.total_trainable_params = int(n_comps*(1+n_dims+0.5*n_dims*(n_dims+1)))\n",
    "#         self.params = param_vec\n",
    "#         assert tf.size(param_vec) == self.total_trainable_params, 'the supplied parameter vector is not commensurate with the n_dims, and n_comps'\n",
    "        \n",
    "#     @property\n",
    "#     def distribution(self):\n",
    "#         # transforming vector in to parameters\n",
    "#         logits,mus,covs,chols = vec2gmm_params(self.ndims,self.ncomps,self.params)\n",
    "#         # Instantiating the bijector\n",
    "#         gmc_bijector = GMC_bijector(self.ndims, self.ncomps, [logits, mus, tf.linalg.diag_part(covs)])\n",
    "#         # Specifying the base distribution\n",
    "#         base_dist = tfd.MixtureSameFamily(tfd.Categorical(logits=logits),\n",
    "#                                           tfd.MultivariateNormalTriL(loc=mus,scale_tril=chols))\n",
    "#         # Instnatiating the gmc distribution as a transformed distribtution\n",
    "#         gmc_dist = tfd.TransformedDistribution(distribution=base_dist,bijector=gmc_bijector)    \n",
    "#         return gmc_dist   \n",
    "    \n",
    "#     @property\n",
    "#     def identifiability_prior(self):\n",
    "#         # transforming vector in to parameters\n",
    "#         logits,mus,covs,_ = vec2gmm_params(self.ndims,self.ncomps,self.params)        \n",
    "#         alphas = tf.math.softmax(logits)\n",
    "#         variances = tf.linalg.diag_part(covs)        \n",
    "#         vec1 = tf.linalg.matvec(tf.transpose(mus),alphas)\n",
    "#         vec2 = tf.linalg.matvec(tf.transpose(variances + mus**2),alphas)\n",
    "#         log_prior_1 = tfd.MultivariateNormalDiag(loc=tf.zeros(self.ndims),scale_diag=1E-1*tf.ones(self.ndims)).log_prob(vec1)\n",
    "#         log_prior_2 = tfd.MultivariateNormalDiag(loc=tf.ones(self.ndims) ,scale_diag=1E-1*tf.ones(self.ndims)).log_prob(vec2)\n",
    "#         return log_prior_1,log_prior_2\n",
    "    \n",
    "\n",
    "    \n",
    "# class GMCM:\n",
    "#     def __init__(self, n_dims, data_in, forward_transform=None, marginals_list=None, gmc=None):\n",
    "        \n",
    "#         self.ndims = n_dims\n",
    "#         self.data_transform = forward_transform\n",
    "#         self.gmc = gmc\n",
    "#         if gmc is not None:\n",
    "#             self.ncomps = gmc.ncomps\n",
    "        \n",
    "#         if forward_transform is not None:\n",
    "#             data_in = forward_transform.inverse(data_in).numpy()\n",
    "#         self.data_in = data_in\n",
    "        \n",
    "#         if marginals_list is None:\n",
    "#             print('Learning Marginals')\n",
    "#             ts = time.time()\n",
    "#             marginals_list = self.learn_marginals()\n",
    "#             print(f'Marginals learnt in {np.round(time.time()-ts,2)} s.') \n",
    "        \n",
    "#         self.marg_dists = marginals_list\n",
    "#         self.marg_bijector = Marginal_transform(self.ndims,self.marg_dists)       \n",
    "        \n",
    "#     @property\n",
    "#     def distribution(self):\n",
    "#         # setting the gmcm distribution as a transformed distribution of gmc_distribution\n",
    "#         gmcm_dist = tfd.TransformedDistribution(distribution=self.gmc.distribution,bijector=self.marg_bijector)\n",
    "#         if self.data_transform is not None:\n",
    "#             gmcm_dist = tfd.TransformedDistribution(distribution=gmcm_dist,bijector=self.data_transform)\n",
    "#         return gmcm_dist\n",
    "    \n",
    "    \n",
    "#     def learn_marginals(self):\n",
    "#         # fitting marginal distributions first\n",
    "#         marg_dist_list=[]\n",
    "#         for j in range(self.ndims):\n",
    "#             input_vector = self.data_in[:,j].reshape(-1,1)\n",
    "#             marg_gmm_obj = GMM_best_fit(input_vector,max_ncomp=10)\n",
    "#             marg_gmm_tfp = tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(probs=marg_gmm_obj.weights_.flatten().astype('float32')),\n",
    "#                                                 components_distribution=tfd.Normal(loc=marg_gmm_obj.means_.flatten().astype('float32'),\n",
    "#                                                                                    scale = marg_gmm_obj.covariances_.flatten().astype('float32')**0.5),)\n",
    "            \n",
    "#             # creating a dictionary containing necessary information about each marginal distribution\n",
    "#             info_dict={'cdf':marg_gmm_tfp.cdf,\n",
    "#                        'log_pdf':marg_gmm_tfp.log_prob,\n",
    "#                        'lb':tf.reduce_min(input_vector)-3*tfp.stats.stddev(input_vector),\n",
    "#                        'ub':tf.reduce_max(input_vector)+3*tfp.stats.stddev(input_vector)                         \n",
    "#                       }\n",
    "            \n",
    "#             marg_dist_list.append(info_dict)\n",
    "        \n",
    "#         return marg_dist_list\n",
    "        \n",
    "#     def init_GMC_params(self,initialization=['random',None]):\n",
    "#         # Initializing the GMC params \n",
    "#         init_method, seed_val = initialization\n",
    "#         if init_method == 'random':\n",
    "#             if seed_val is not None:\n",
    "#                 np.random.seed(seed_val)\n",
    "#             alphas = tf.ones(self.ncomps)/self.ncomps\n",
    "#             mus = tf.constant(np.random.randn(self.ncomps,self.ndims).astype('float32'))\n",
    "#             covs = tf.repeat(tf.expand_dims(tf.eye(self.ndims),0),self.ncomps,axis=0)\n",
    "#         elif init_method == 'gmm':            \n",
    "#             gmm = mixture.GaussianMixture(n_components=self.ncomps,covariance_type='full',max_iter=1000,n_init=5)\n",
    "#             gmm.fit(self.data_in)\n",
    "#             alphas = gmm.weights_.astype('float32')\n",
    "#             mus = gmm.means_.astype('float32')\n",
    "#             covs = gmm.covariances_.astype('float32')                                                            \n",
    "        \n",
    "#         # changing the parameters to standardize the resulting gmm\n",
    "#         alphas,mus,covs = standardize_gmm_params(alphas,mus,covs)\n",
    "#         # now initializing trainable parameters\n",
    "#         init_params = tf.Variable(gmm_params2vec(self.ndims,self.ncomps,alphas,mus,covs))\n",
    "        \n",
    "#         return init_params\n",
    "    \n",
    "    \n",
    "#     def fit_GMC_dist(self, n_comps, optimizer = tf.optimizers.Adam(learning_rate=1E-2), initialization = ['random',None], max_iters = 1000, batch_size = 10, print_interval=100, regularize=True, plot_results = False):\n",
    "#         self.ncomps = n_comps\n",
    "#         # getting the marginal CDF values\n",
    "#         u_mat = self.marg_bijector.inverse(self.data_in)\n",
    "#         # initializing the parameters\n",
    "#         gmc_params = self.init_GMC_params(initialization=initialization)\n",
    "#         # instantiation GMC object\n",
    "#         gmc_obj = GMC(self.ndims,self.ncomps,gmc_params)\n",
    "        \n",
    "#         # Defining the training step\n",
    "#         @tf.function\n",
    "#         def train_step(u_selected):\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 neg_gmc_ll = -tf.reduce_mean(gmc_obj.distribution.log_prob(u_selected))\n",
    "#                 ident_prior = gmc_obj.identifiability_prior\n",
    "#                 if regularize:\n",
    "#                     total_cost = neg_gmc_ll - tf.reduce_sum(ident_prior)\n",
    "#                 else:\n",
    "#                     total_cost = neg_gmc_ll\n",
    "                    \n",
    "#             grads = tape.gradient(total_cost, gmc_obj.params)\n",
    "#             if not (tf.reduce_any(tf.math.is_nan(grads)) or tf.reduce_any(tf.math.is_inf(grads))):\n",
    "#                 optimizer.apply_gradients(zip([grads], [gmc_obj.params])) #updating the gmc parameters\n",
    "#             return neg_gmc_ll,ident_prior[0],ident_prior[1]\n",
    "\n",
    "#         neg_ll_trn = np.empty(max_iters)  \n",
    "#         neg_ll_trn[:] = np.NaN\n",
    "#         neg_prior_1 = np.empty(max_iters)  \n",
    "#         neg_prior_1[:] = np.NaN\n",
    "#         neg_prior_2 = np.empty(max_iters)  \n",
    "#         neg_prior_2[:] = np.NaN\n",
    "#         np.random.seed(10)\n",
    "#         ts = time.time() # start time\n",
    "#         # Optimization iterations\n",
    "#         for itr in np.arange(max_iters):\n",
    "#             np.random.seed(itr)\n",
    "#             # Executing a training step\n",
    "#             samps_idx = np.random.choice(u_mat.shape[0],batch_size)\n",
    "#             u_selected_trn = tf.gather(u_mat,samps_idx)\n",
    "#             out = train_step(u_selected_trn)\n",
    "#             neg_ll_trn[itr] = out[0].numpy()\n",
    "#             neg_prior_1[itr] = out[1].numpy()\n",
    "#             neg_prior_2[itr] = out[2].numpy()    \n",
    "#             # Printing results every 100 iteration    \n",
    "#             if tf.equal(itr%print_interval,0) or tf.equal(itr,0):\n",
    "#                 time_elapsed = np.round(time.time()-ts,1)\n",
    "#                 print(f'@ Iter:{itr}, Training error: {neg_ll_trn[itr]}, LogPriors: {np.round(neg_prior_1[itr],2), np.round(neg_prior_2[itr],2)}, Time Elapsed: {time_elapsed} s')    \n",
    "        \n",
    "#         if plot_results:\n",
    "#             # Plotting results\n",
    "#             plt.plot(neg_ll_trn)\n",
    "#             plt.xlabel('Iteration',fontsize=12)\n",
    "#             plt.ylabel('Neg_logLike',fontsize=12)\n",
    "#             plt.legend(['train'],fontsize=12)\n",
    "        \n",
    "#         # setting gmc distritbution embedded inside GMCM\n",
    "#         self.gmc = gmc_obj\n",
    "         \n",
    "#         return neg_ll_trn\n",
    "    \n",
    "#     def get_marginal(self,dim_list):        \n",
    "#         data_in_new = tf.gather(self.data_in,dim_list,axis=1).numpy()\n",
    "#         logits,mus,covs,_ = vec2gmm_params(self.ndims,self.ncomps,self.gmc.params)\n",
    "#         alphas = tf.math.softmax(logits)\n",
    "#         dim_remove = list(set(list(range(self.ndims)))-set(dim_list))\n",
    "#         mus_new = tf.gather(mus, dim_list, axis=1)\n",
    "#         covs_new = tf.TensorArray(tf.float32,self.ncomps)\n",
    "#         for k in range(self.ncomps):\n",
    "#             temp_mat = covs[k].numpy()\n",
    "#             covs_new = covs_new.write(k,temp_mat[np.ix_(dim_list,dim_list)])\n",
    "#         covs_new = covs_new.stack()\n",
    "#         # getting the gmc object first for the marginal gmcm\n",
    "#         marginal_gmc_params = gmm_params2vec(len(dim_list),self.ncomps,alphas,mus_new,covs_new)\n",
    "#         marg_gmc = GMC(len(dim_list),self.ncomps,marginal_gmc_params)\n",
    "#         # then getting the marginals along the specified dimensions\n",
    "#         marg_list_new = []\n",
    "#         for j in range(self.ndims):\n",
    "#             if j in dim_list:\n",
    "#                 marg_list_new.append(self.marg_dists[j])\n",
    "#         # creating the marginal gmcm object\n",
    "#         marg_gmcm_dist = GMCM(len(dim_list), data_in_new, forward_transform=self.data_transform, marginals_list=marg_list_new, gmc=marg_gmc)\n",
    "#         return marg_gmcm_dist   \n",
    "    \n",
    "#     def get_conditional(self,obs_dim_list, value_list):\n",
    "        \n",
    "#         x_obs = np.array(value_list).reshape(1,-1).astype('float32')\n",
    "#         unobs_dim_list = list(set(range(self.ndims)) - set(obs_dim_list))\n",
    "        \n",
    "#         #Obtaining the marginal distribution for the observed and missing part\n",
    "#         gmcm_observed = self.get_marginal(obs_dim_list)\n",
    "#         gmcm_unobserved = self.get_marginal(unobs_dim_list)\n",
    "        \n",
    "#         temp_obj = gmcm_observed\n",
    "#         z_obs = np.copy(x_obs)\n",
    "#         while hasattr(temp_obj.distribution,'bijector'):\n",
    "#             z_obs = temp_obj.distribution.bijector.inverse(z_obs).numpy()\n",
    "#             temp_obj = temp_obj.distribution\n",
    "\n",
    "#         #Obtaining the conditional mu and Sigma of individual compoents of the missing part given the data of observed part\n",
    "#         mus_cond = np.zeros((self.ncomps,len(unobs_dim_list))).astype('float32')\n",
    "#         covs_cond = np.zeros((self.ncomps,len(unobs_dim_list), len(unobs_dim_list))).astype('float32')\n",
    "#         logits_cond = np.zeros(self.ncomps).astype('float32')\n",
    "        \n",
    "#         logits,mus,covs,_ = vec2gmm_params(self.ndims,self.ncomps,self.gmc.params)\n",
    "#         logits_unobs,mus_unobs,covs_unobs,_ = vec2gmm_params(gmcm_unobserved.ndims,gmcm_unobserved.ncomps,gmcm_unobserved.gmc.params)\n",
    "#         logits_obs,mus_obs,covs_obs,_ = vec2gmm_params(gmcm_observed.ndims,gmcm_observed.ncomps,gmcm_observed.gmc.params)\n",
    "        \n",
    "#         for k in range(self.ncomps):\n",
    "#             sig_11 = covs_unobs.numpy()[k]\n",
    "#             sig_22 = covs_obs.numpy()[k]\n",
    "#             sig_12 = covs.numpy()[k][np.ix_(unobs_dim_list,obs_dim_list)]\n",
    "#             sig_21 = sig_12.T\n",
    "#             mu_11 = mus_unobs[k,:]\n",
    "#             mu_22 = mus_obs[k,:]\n",
    "            \n",
    "# #             temp_mat1 = np.concatenate([np.concatenate([sig_11,sig_12],axis=1),np.concatenate([sig_21,sig_22],axis=1)],axis=0)\n",
    "# #             temp_mat2=covs.numpy()[k]\n",
    "# #             lll = unobs_dim_list+obs_dim_list\n",
    "# #             temp_mat2 = temp_mat2[:,lll]\n",
    "# #             temp_mat2 = temp_mat2[lll,:]\n",
    "# #             print(temp_mat1-temp_mat2)\n",
    "            \n",
    "\n",
    "#             # Getting the conditional mu and Sigma\n",
    "#             mu_bar = mu_11 + np.matmul(sig_12,  np.linalg.solve(sig_22,z_obs.T)).flatten()\n",
    "#             sig_bar = sig_11 - np.matmul(sig_12,  np.linalg.solve(sig_22,sig_21))\n",
    "#             mus_cond[k] = mu_bar\n",
    "#             covs_cond[k] = (sig_bar+sig_bar.T)/2\n",
    "\n",
    "#             # Getting the log proability of the components conditioned on the observed data\n",
    "#             logits_cond[k] = logits_obs[k] + tfd.MultivariateNormalFullCovariance(loc=mus_obs[k],\n",
    "#                                                                                          covariance_matrix=covs_obs[k]).log_prob(z_obs)\n",
    "#         #logits to probabilities\n",
    "#         alphas_cond = tf.math.softmax(logits_cond)\n",
    "#         # parameter vector of the conditional gmc distribution\n",
    "#         conditional_gmc_params = gmm_params2vec(len(unobs_dim_list),self.ncomps,alphas_cond,mus_cond,covs_cond)\n",
    "#         cond_gmc = GMC(len(unobs_dim_list),self.ncomps,conditional_gmc_params)\n",
    "#         # then getting the marginals along the specified dimensions\n",
    "#         marg_list_new = []\n",
    "#         for j in range(self.ndims):\n",
    "#             if j in unobs_dim_list:\n",
    "#                 marg_list_new.append(self.marg_dists[j])\n",
    "#         # creating the conditional gmcm object\n",
    "#         data_in_new = tf.gather(self.data_in,unobs_dim_list,axis=1).numpy()\n",
    "#         cond_gmcm_dist = GMCM(len(unobs_dim_list), data_in_new, forward_transform=self.data_transform, marginals_list=marg_list_new, gmc=cond_gmc)\n",
    "#         return cond_gmcm_dist  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting marginal distributions first\n",
    "marg_dist_list=[]\n",
    "for j in range(self.ndims):\n",
    "    marg_dist_tfp=tfd.Gamma(concentration=tf.Variable(2.),rate=tf.Variable(0.5))\n",
    "    # creating a dictionary containing necessary information about each marginal distribution\n",
    "    info_dict={'cdf':marg_dist_tfp.cdf,\n",
    "               'icdf':marg_dist_tf.quantile,\n",
    "               'log_pdf':marg_dist_tfp.log_prob,\n",
    "               'lb':0.,\n",
    "               'ub':np.inf}\n",
    "            \n",
    "marg_dist_list.append(info_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md=tfd.Gamma(concentration=tf.Variable(2.),rate=tf.Variable(0.5))\n",
    "md.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.quantile(0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict={'wdf':3,'sdf':4}\n",
    "'wdf' in info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
